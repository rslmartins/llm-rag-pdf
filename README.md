# 🧠 Knowledge Management System with LLM + RAG

This project is a **knowledge management system** that leverages **LLM (Large Language Model) with Retrieval-Augmented Generation (RAG)** to extract insights from user-uploaded PDFs. It uses a simple and interactive **Streamlit** web interface.

# ⚠️ Disclaimer

Make sure your PDFs don't contain sensitive information, since Google's Gemini uses external LLM APIs.

## 🔧 Technologies Used

### 🖼️ Streamlit
A lightweight and powerful framework for building interactive data apps in Python, which is used to:

- 📂 Provide a clean and simple interface to upload your PDF documents.
- 🔍 Let users ask questions in natural language about the uploaded content.
- 💾 Automatically detect and load existing data (like FAISS indexes) related to uploaded PDFs.
- 🧠 Show answers generated by the language model, based on the most relevant information from the documents.

### 📄 PyPDF2
A PDF library for Python to read and extract text from PDF files.

- Parses the uploaded PDFs to extract text content that will later be embedded and stored.

### 🧠 LangChain
A powerful framework for building applications with language models.

- Used to manage the RAG pipeline:
  - Text loading and splitting
  - Embedding generation
  - Vector store integration
  - Question-answering using LLM

### 🧬 FAISS (Meta's AI Similarity Search)
A library for efficient similarity search and clustering of dense vectors.

- Stores and load text embeddings from the PDF.
- Performs fast similarity searches to retrieve relevant chunks during Q&A.

### 🌟 Gemini (Google AI Studio API)  
A family of large language models developed by Google, accessible via the Google AI Studio API.

- Serves as the backend LLM to process user queries and generate intelligent responses.
- Integrated via LangChain to provide high-quality, context-aware answers from the vector database.

## 🔐 API Key Setup

This app uses the **Google AI Studio** for language model queries. You need an API key:

1. Get your key from: [Google AI Studio](https://aistudio.google.com/apikey)
2. Create a `.env` file in the root directory with the following content: `GOOGLE_API_KEY=your_api_key_here`

## 🧰 Set up the virtual environment and install dependencies

1. Create virtual environment `python -m venv venv`
2. Activate virtual environment on Windows `venv\Scripts\activate` or On macOS/Linux `source venv/bin/activate`
3. Install required packages `pip install -r requirements.txt`

## ▶️ How to run the application locally
1. Activate virtual environment on Windows `venv\Scripts\activate` or On macOS/Linux `source venv/bin/activate`
2. Run the application `streamlit run app.py`
3. Go into your browser to load through your browser the [application](http://localhost:8501/)

## 📚 References
- [Build a Q&A App for a webpage](https://swethag04.medium.com/build-a-q-a-app-for-a-webpage-431b7b8220e6)
- [Mastering Prompt Templates in LangChain](https://medium.com/@abdullah.iu.cse/mastering-prompt-templates-in-langchain-3f48fa92327f)
- [Building a Private Data-Driven Question-Answering System with Large Language Models](https://medium.com/@nbasatish/building-a-private-data-driven-question-answering-system-with-large-language-models-a0b4d4c2385c)
- [Build a Simple LLM Application with LCEL](https://python.langchain.com/v0.2/docs/tutorials/llm_chain/)
- [Quickstart (Local) | Gen AI Toolbox for Databases](https://googleapis.github.io/genai-toolbox/getting-started/local_quickstart/)
